<h2>Why moving signs between code points is harmful</h2>

Since Unicode 5.2, released in 2009, a large collection of fonts, tools, encodings, data bases, and teaching materials
have been created
that all assume the signs as assigned to code points in Unicode 5.2. 
There are more than half a dozen publicly released hieroglyphic fonts by now, and possibly more that
were not publicly released.
Many encodings of texts have been created with tools that use one of these fonts.
These fonts and these encodings may be 
distributed and redistributed on the web, without any central control.
<p>
Now suppose a sign is moved to a different code point, and the old code point is given a new sign. 
It then immediately becomes uncertain what the intention is of that old code point.
For a new encoding, it depends on the encoder, who needs to make sure they use an up-to-date version of a
hieroglyphic text processing tool. In turn it depends on the developer of that tool, 
who has to make sure they have an up-to-date version of the font.
In turn it depends on the font creator, who has to check an up-to-date version of the Unicode code charts
for any changes. (With the sign list now having grown from formerly just over 1000 signs to about 5000 signs,
this is quite a challenge in itself.)
Any existing encoding immediately loses its validity, as the code points will no longer reflect 
the intentions of the encoder. The encoder will need to revisit their encodings and make corrections.
Also databases and teaching materials need to be updated. This is the most optimistic case, assuming the encoders and
creators of databases and teaching materials are still available to make the necessary updates.
<p>
Naively one could argue that the date of creation of an encoding determines how code points are to be
interpreted. If it was created before the change was made, then it is the original interpretation of the code points.
If it was created one or two years after the change to the code charts, after hopefully all fonts and tools had been
updated (which may be overly optimistic), then it is the new interpretation of the code points.
The problem is that one cannot see from an encoding when it was created. A web page containing hieroglyphic
encodings created in the past does not have a special label saying "this page was created before 2024 when the Unicode
sign list was upended".
<p>
In short, if the aim is to be able to distinguish two glyphs, which are not to be confused
any more in future encodings, then the first step should <b>not</b> be to assign one glyph to the code point that 
hitherto belonged to the other, as this creates the confusion that one aimed to avoid.
<p>
In the Database, there appear to be vague references to some graphical variants being less common than others
and I'm guessing (and no more than guessing) 
that the underlying assumption was that Gardiner chose a rare or atypical graphical variant to represent a
grapheme, and that is why in the past when the code point was used, the
encoder "really" intended the more common variant. I don't think that holds up to scrutiny.
In the case of the pole-on-block versus pole-over-block for example (see U+132BC above), I strongly
suspect the evidence was misinterpreted. Moreover, second-guessing the "real" intentions of
some hypothetical encoder of some hypothetical inscription in the past is a precarious undertaking. One cannot know under what circumstances
existing encodings were created. Some graphical variants may be more common for some periods than for others.
And perhaps the encoder intended to digitize Gardiner's grammar rather than encode a randomly chosen
inscription from a random period, and so definitely intended Gardiner's glyphs and not the new glyphs assigned to the 
old code points.
<p>
One more problem that arises when signs are moved may have been overlooked.
Consider a line in the UniKemet database like:
<blockquote>
U+143C7 kEH_JSesh   X4E
</blockquote>
This means that JSesh X4E is tied to code point U+143C7.  This JSesh X4E is not some platonic idea of a character that could have different graphical realizations, it is one concrete glyph that exists in the font of the JSesh editor. (It is ironic that JSesh names are now treated as more immutable than Unicode code points, even though in the decades of JSesh's existence, the font has been subjected to many rounds of improvements; cf. <a href="https://jsesh.qenherkhopeshef.org/signdiffs">Sign Differences Across JSesh Versions</a>. Therefore X4E may not be the same glyph as it was 5 years ago or will be in 5 years from now. But this aside.)
<p>
What to do if the JSesh X4E glyph moves to a different code point, say in Unicode 25 ? The obvious answer is: the kEH_JSesh X4E entry will move with the glyph to the new code point.
<p>
Now consider UniKemet names. These must similarly stand for glyphs, not for code points or abstract platonic ideas of characters. In fact, just like JSesh names stand for glyphs in JSesh encodings, UniKemet names are used to denote glyphs in <a href="https://mjn.host.cs.st-andrews.ac.uk/egyptian/res/">RES hieroglyphic encodings</a>, albeit without zero padding. 
To be consistent, if the glyph that used to be at U+132BC is moved to U+14107, then we should change:
<blockquote>
U+132BC kEH_UniK    R010A
</blockquote>
to:
<blockquote>
U+14107 kEH_UniK    R010A
</blockquote>
Currently, if I look in the UniKemet database under U+14107 I see nothing of the sort. It was given a fresh, unrelated UniKemet name R010H and there is nothing that reminds us that the reason this glyph was introduced in Unicode in the first place was because of Gardiner (1928), who called it R10*. It is ironic that the UniKemet database links code points to various sign lists, but not to Gardiner's sources that were the primary reason for their existence.
But if we update the UniKemet database to link U+14107 to the glyph R010A, then we still have the Unicode name "EGYPTIAN HIEROGLYPH R010A" immutably (!) linked to U+132BC.  In other words, the Unicode names and the UniKemet names will now start to diverge, and this will be for just a handful of signs, while for the majority of the 1072 basic signs, the Unicode names and UniKemet names perfectly align. For the sake of completeness these are the moved code points:
<table>
<tbody>
<tr><th>old</th><th>new</th></tr>
<tr><td>U+13108</td><td>U+13B83</td></tr>
<tr><td>U+13163</td><td>U+13C4A</td></tr>
<tr><td>U+1324A</td><td>U+13CA7</td></tr>
<tr><td>U+13277</td><td>U+13FDC</td></tr>
<tr><td>U+132BC</td><td>U+14107</td></tr>
</tbody>
</table>
and then there are two more special cases that are less clear-cut:
<table>
<tbody>
<tr><th>old</th><th>new</th></tr>
<tr><td>U+1315D</td><td>U+13C5D</td></tr>
<tr><td>U+1334C</td><td>U+1334B</td></tr>
</tbody>
</table>
<p>
The divergence of UniKemet names from Unicode names would add one more naming headache to the various other naming headaches that already exist.
<p>
Moving signs to other code points has many disadvantages due to existing encodings, existing databases, existing fonts, and existing hieroglyphic processing tools. All of these disadvantages are severe and will continue to haunt the user community for years to come. The above naming issue is one more such disadvantage. On the flip side, there are no advantages. All perceived advantages of moving signs do not hold
up to scrutiny and are outweighed by far by the severe disadvantages.
<p>
Why not reverse course while it is still possible?
<p>
In the end, there is also the general Encoding Stability policy of Unicode:
<blockquote>
"Once a character is encoded, it will not be moved or removed." 
</blockquote>
[ <a href="https://www.unicode.org/policies/stability_policy.html">https://www.unicode.org/policies/stability_policy.html</a> ]
<p>
No doubt one can cite rare examples from the long history of Unicode where this policy was not strictly adhered to due
to exceptional circumstances and significant overriding concerns, in the absence of any real disruptions that this might cause.
But for the cases listed above, 
there are no exceptional circumstances or overriding concerns,
and all of this was entirely unnecessary.
As a long-time user, I certainly experienced what happened to the basic list as highly disruptive,
notwithstanding those who never used the basic list in Unicode  
for their own work, who have no stake in the stability of the basic list,
and who may wish to downplay the issue.

<h2 id="overlays">Atomic glyphs of overlays</h2>

In the case of, for example, U+130C1 (forearm and leg overlayed; see above), 
one will find inscriptions where the forearm is depicted as being over the leg, 
inscriptions where the leg is depicted as being over the forearm, as well 
as inscriptions where the "z-order" is indeterminate, that is, where 
the curves of the forearm and leg simply cross one another.
This is a palaeographic matter, and therefore Unicode appropriately does not attempt to
encode these different graphical realizations, 
and the single code point U+130C1 is meant to cover all three cases.
<p>
Nonetheless, there are advantages to keeping the z-order of glyphs 
in the code charts of such composite
glyphs unchanged (in the absence of any compelling reasons for any change). 
This has to do with hieroglyphic processing
tools that are built on top of the Unicode sign list 
and that provide support for a more faithful specification of the positioning of
signs in a hieroglyphic inscription than would ever be possible in Unicode. I'm in particular referring to
<a href="https://mjn.host.cs.st-andrews.ac.uk/egyptian/res/">RES</a>, which was
mentioned before. It allows scaling of signs, fine-tuning of distances between signs, 
arbitrary degrees of rotation, and so on.
RES also allows specification of z-order of overlays in the case of compositional encoding. 
The reason RES can do this is because its three implementations are in general-purpose
programming languages (C, Java, JavaScript), in which one can implement graphical algorithms
that are beyond the power of typical Unicode technology such as OpenType.
For example, the Unicode 5.2 glyph of U+130C1 can be expressed in RES as:
<blockquote>
stack[on](D36,D58)
</blockquote>
whereas its Unicode 16 glyph corresponds to:
<blockquote>
stack[under](D36,D58)
</blockquote>
The indeterminate form, where conceptually the two signs have the same z-order, 
is expressed by:
<blockquote>
stack(D36,D58)
</blockquote>
Because U+130C1 has an individual name D59 for the atomically encoded overlay, derived from the
UniKemet names of the Unicode 5.2 set, the first of these could alternatively be encoded as:
<blockquote>
D59
</blockquote>
Regrettably, with a font created according to the Unicode 16 code charts,
D59 has now become equivalent to stack[under](D36,D58) instead.
It is very likely that some RES encodings created in the past will thereby have become 
inconsistent with the intentions of the encoders.
<p>
I don't see any downsides to reverting to the Unicode 5.2 forms for composite signs, 
all the more as the Database offers no evidence in favour of any of
the variant graphical realizations with different z-orders, implying that 
there was never any compelling reason to depart from the Unicode 5.2 forms to begin with.
Moreover, composite signs such as U+130C1 were declared "Legacy" in favour of compositional
encoding using the OVERLAY control, and therefore the glyphs of such signs will not affect
future users who intend to avoid legacy signs altogether.
<p>
Incidentally, the motivation for designing RES was to address the problem that
the longevity of hieroglyphic encodings is very limited with
traditional means, and changing to a different font, 
changing to a different tool, or even changing to a different version of the same tool
often meant that an existing encoding lost its validity.
This has arguably plagued Egyptology for decades. For background see
<a href="https://mjn.host.cs.st-andrews.ac.uk/publications/2013b.pdf">The Manuel de Codage encoding of hieroglyphs impedes development of corpora</a> and
<a href="https://mjn.host.cs.st-andrews.ac.uk/publications/2021a.pdf">Formatting of Ancient Egyptian hieroglyphic text</a>.
<p>
The reason why RES was designed on top of Unicode was because at the time
it was anticipated that Unicode would provide better stability than the various versions
of the Manuel de Codage signs lists and the Hieroglyphica and their implementations
in various tools. The trust that I placed in Unicode at the time
was that it would not fundamentally change characters without compelling reasons at the very least,
in the light of Unicode's stability policies. I would also expect a careful analysis of the disruption
that any changes cause, weighing these against any perceived advantages of these changes.

<h2>Closing comments</h2>

It has been suggested that I hold the view that glyphs in the code charts should remain 100% immutable
under all circumstances.
If this were true, this could be used to dismiss everything I've written here.
However, this is a gross mischaracterization and misses the nuances of the discussion
above.
I do not oppose changes per se, I oppose changes whose disadvantages for typical users far outweigh 
the advantages (if there are any). 
Moreover, much of my criticism of the extended list concerns an overemphasis
on palaeographic details, which is quite the opposite of claiming that each minute detail
of each glyph is significant. I think in particular that the extended list has been created 
with too little awareness of the potential variability of glyphs in different fonts.
There are cases where subtle differences in appearance are encoded in multiple code points
when such differences are likely to vanish within the margins of this variability,
and thereby become meaningless on the level of Unicode. 
This is not because any font is "wrong", but
because the exact angle of a staff held by an old man (cf. U+13017 and U+134CD)
or the exact relative sizes of elements within a complex glyph 
(cf. U+13A01 versus U+13A02, U+13C9D versus U+13C9F, U+13F98 versus U+13F99, U+14027 versus U+14028, U+14067 versus U+1406E, etc.)
would not normally be reproduced exactly by the creator of a new font.
<p>
It should here be understood that Unicode is not a tool like JSesh that has a fixed font.
There are currently more than half a dozen publicly available Unicode fonts for the basic list,
and within the next few years some of these wil doubtless also have the extended set.
This means one cannot expect to see the exact same glyphs regardless of the tool or the 
platform one is using. For a Unicode sign list to be practicable,
at least two conditions must therefore be satisfied:
<uL>
<li> The glyphs in the code charts plus ancillary documentation should provide enough
guidance for font creators to know how to correctly draw glyphs.
<li> There should not be multiple code points for the same underlying grapheme when
the code charts and the ancillary documentation cannot reasonably and unambiguously
distinguish them.
</ul>
<p>
I would also again warn against confusing Ancient Egyptian with any other writing system.
Just because a certain approach to developing CJK character sets worked well in the past doesn't
mean the same approach would work well or at all for Ancient Egyptian. Just because certain
changes to CJK code charts didn't cause disruption to users doesn't
mean that any changes to code charts of hieroglyphs don't cause disruption.
Ancient Egyptian has its own peculiarities, which should be carefully considered when maintaining
existing code points and when introducing new code points.
<p>
A claim has furthermore been made that Unicode cannot become a tool for Egyptological research 
unless the sign list of hieroglyphs encodes arbitrarily small palaeographic details.
The reasoning goes that such details could potentially
convey information that is relevant for dating or for otherwise analyzing the 
extra-linguistic properties of an inscription, and therefore one cannot afford to abstract 
away from those details.
However, if one takes that position to its logical extreme, then encoding an inscription 
would be akin to producing a facsimile, with each element of the encoding representing a token
rather than a type, as each nuance of the appearance of a token
could potentially carry additional information.
This however would hinder rather than help most types of research including
palaeographic research. In particular, most research questions involving hieroglyphic texts
require the ability to find occurrences of graphemes in a corpus
and this becomes difficult if a grapheme
is represented not by one code point, but by dozens of code points for minor graphical
variants.
Moreover, once graphical variants become near indistinguishable, the choice
between one code point and another becomes arbitrary, which introduces noise
into the process of encoding. 
Noise is not information and generally makes it harder to answer research questions.
<p>
(See also my <a href="unicode16comments.html">comments on Unicode 16</a>
for examples of palaeographic details that turned out to be arbitrary and that did not deserve
to be elevated to the formal status they were given.)
<p>
Overall, the developments in Unicode 16 seem to lack a direction and purpose.
On the one hand, there seems to have been the pressure from the side of the 
Unicode Consortium to introduce
large numbers of new code points all at once, which necessarily went at the expense of
quality. Only so much could reasonably be achieved in such a short time frame for so many signs,
even by a highly dedicated team of domain experts.
One justification that has been given for this push for quantity over quality
is that urgently needed digitization projects cannot continue until the
Unicode sign list of hieroglyphs is "complete". (The notion of "completeness" is questionable
to say the least when dealing with an open-ended writing system, but this aside.)
<p>
On the other hand, what was actually done for the basic and extended lists
to some extent moves away from the above-stated aims.
For one thing, the basic list from Unicode 5.2 was adequate for digitizing any publication
from the Griffith Institute that used Gardiner's font. With the "revision" of
the basic list in Unicode 16, this has become more problematic. 
If the aim is to increase the <i>coverage</i> of the sign list, allowing a larger
proportion of inscriptions, published or unpublished, to be digitized, then
one should <b>not</b> unduly restrict the applicability of each code point
through a narrow definition that seems to characterize an individual token rather than a
type. But exactly this was done in Unicode 16, not just for the extended list
but also for the basic list. Several code points in the range of the Unicode 5.2
hieroglyphs are now so narrowly (re)defined that they no long cover the signs from
Gardiner's grammar.
<p>
The central question is what one aims to achieve.
Is it to increase the 
coverage of the sign list to allow digitization of a larger proportion of
texts? If so, then surely this can be done in a responsible and sound way, to benefit
everyone, for grammatical, lexicographical and other use. This includes palaeographical applications,
and the requirement that new signs be attested and be
documented by domain experts before they enter Unicode is an entirely reasonable one, and something I have long advocated.
Or alternatively is Unicode to become primarily a tool for gathering raw palaeographic data,
with signs being moved around and code points being redefined every so often if this
suits vaguely specified short-term aims of reorganizing that raw data?
If so, then this is to the detriment of all other users.
