<h2>Why moving signs between code points is harmful</h2>

Since Unicode 5.2, released in 2009, a large collection of fonts, tools, encodings, data bases, and teaching materials
have been created
that all assume the signs as assigned to code points in Unicode 5.2. 
There are more than half a dozen publicly released hieroglyphic fonts by now, and possibly more that
were not publicly released.
Many encodings of texts have been created with tools that use one of these fonts.
These fonts and these encodings may be 
distributed and redistributed on the web, without any central control.
<p>
Now suppose a sign is moved to a different code point, and the old code point is given a new sign. 
It then immediately becomes uncertain what the intention is of that old code point.
For a new encoding, it depends on the encoder, who needs to make sure they use an up-to-date version of a
hieroglyphic text processing tool. In turn it depends on the developer of that tool, 
who has to make sure they have an up-to-date version of the font.
In turn it depends on the font creator, who has to check an up-to-date version of the Unicode code charts
for any changes. (With the sign list now having grown from formerly just over 1000 signs to about 5000 signs,
this is quite a challenge in itself.)
Any existing encoding immediately loses its validity, as the code points will no longer reflect 
the intentions of the encoder. The encoder will need to revisit their encodings and make corrections.
Also databases and teaching materials need to be updated. This is the most optimistic case, assuming the encoders and
creators of databases and teaching materials are still available to make the necessary updates.
<p>
Naively one could argue that the date of creation of an encoding determines how code points are to be
interpreted. If it was created before the change was made, then it is the original interpretation of the code points.
If it was created one or two years after the change to the code charts, after hopefully all fonts and tools had been
updated (which may be overly optimistic), then it is the new interpretation of the code points.
The problem is that one cannot see from an encoding when it was created. A web page containing hieroglyphic
encodings created in the past does not have a special label saying "this page was created before 2024 when the Unicode
sign list was upended".
<p>
In short, if the aim is to be able to distinguish two glyphs, which are not to be confused
any more in future encodings, then the first step should <b>not</b> be to assign one glyph to the code point that 
hitherto belonged to the other, as this creates the confusion that one aimed to avoid.
<p>
In the Database, there appear to be vague references to some graphical variants being less common than others
and I'm guessing (and no more than guessing) 
that the underlying assumption was that Gardiner chose a rare or atypical graphical variant to represent a
grapheme, and that is why in the past when the code point was used, the
encoder "really" intended the more common variant. I don't think that holds up to scrutiny.
In the case of the pole-on-block versus pole-over-block for example (see U+132BC above), I strongly
suspect the evidence was misinterpreted. Moreover, second-guessing the "real" intentions of
some hypothetical encoder of some hypothetical inscription in the past is a precarious undertaking. One cannot know under what circumstances
existing encodings were created. Some graphical variants may be more common for some periods than for others.
And perhaps the encoder intended to digitize Gardiner's grammar rather than encode a randomly chosen
inscription from a random period, and so definitely intended Gardiner's glyphs and not the new glyphs assigned to the 
old code points.
<p>
One more problem that arises when signs are moved may have been overlooked.
Consider a line in the UniKemet database like:
<blockquote>
U+143C7 kEH_JSesh   X4E
</blockquote>
This means that JSesh X4E is tied to code point U+143C7.  This JSesh X4E is not some platonic idea of a character that could have different graphical realizations, it is one concrete glyph that exists in the font of the JSesh editor. (It is ironic that JSesh names are now treated as more immutable than Unicode code points, even though in the decades of JSesh's existence, I suspect the font has been subjected to many rounds of improvements and I have no idea whether X4E is the same glyph as it was 5 years ago or will be in 5 years from now. But this aside.)
<p>
What to do if the JSesh X4E glyph moves to a different code point, say in Unicode 25 ? The obvious answer is: the kEH_JSesh X4E entry will move with the glyph to the new code point.
<p>
Now consider UniKemet names. These must similarly stand for glyphs, not for code points or abstract platonic ideas of characters. In fact, just like JSesh names stand for glyphs in JSesh encodings, UniKemet names are used to denote glyphs in <a href="https://mjn.host.cs.st-andrews.ac.uk/egyptian/res/">RES hieroglyphic encodings</a>, albeit without zero padding. 
To be consistent, if the glyph that used to be at U+132BC is moved to U+14107, then we should change:
<blockquote>
U+132BC kEH_UniK    R010A
</blockquote>
to:
<blockquote>
U+14107 kEH_UniK    R010A
</blockquote>
Currently, if I look in the UniKemet database under U+14107 I see nothing of the sort. It was given a fresh, unrelated UniKemet name R010H and there is nothing that reminds us that the reason this glyph was introduced in Unicode in the first place was because of Gardiner (1928), who called it R10*. It is ironic that the UniKemet database links code points to various sign lists, but not to Gardiner's sources that were the primary reason for their existence.
But if we update the UniKemet database to link U+14107 to the glyph R010A, then we still have the Unicode name "EGYPTIAN HIEROGLYPH R010A" immutably (!) linked to U+132BC.  In other words, the Unicode names and the UniKemet names will now start to diverge, and this will be for just a handful of signs, while for the majority of the 1072 basic signs, the Unicode names and UniKemet names perfectly align. For the sake of completeness these are the moved code points:
<table>
<tbody>
<tr><th>old</th><th>new</th></tr>
<tr><td>U+13108</td><td>U+13B83</td></tr>
<tr><td>U+13163</td><td>U+13C4A</td></tr>
<tr><td>U+1324A</td><td>U+13CA7</td></tr>
<tr><td>U+13277</td><td>U+13FDC</td></tr>
<tr><td>U+132BC</td><td>U+14107</td></tr>
</tbody>
</table>
and then there are two more special cases that are less clear-cut:
<table>
<tbody>
<tr><th>old</th><th>new</th></tr>
<tr><td>U+1315D</td><td>U+13C5D</td></tr>
<tr><td>U+1334C</td><td>U+1334B</td></tr>
</tbody>
</table>
<p>
The divergence of UniKemet names from Unicode names would add one more naming headache to the various other naming headaches that already exist.
<p>
Moving signs to other code points has many disadvantages due to existing encodings, existing databases, existing fonts, and existing hieroglyphic processing tools. All of these disadvantages are severe and will continue to haunt the user community for years to come. The above naming issue is one more such disadvantage. On the flip side, there are no advantages. All perceived advantages of moving signs do not hold
up to scrutiny and are outweighed by far by the severe disadvantages.
<p>
Why not reverse course while it is still possible?
<p>
In the end, there is also the general Encoding Stability policy of Unicode:
<blockquote>
"Once a character is encoded, it will not be moved or removed." 
</blockquote>
[ <a href="https://www.unicode.org/policies/stability_policy.html">https://www.unicode.org/policies/stability_policy.html</a> ]
<p>
No doubt one can cite rare examples from the long history of Unicode where this policy was not strictly adhered to due
to exceptional circumstances and significant overriding concerns, in the absence of any real disruptions that this might cause.
But for the cases listed above, 
there are no exceptional circumstances or overriding concerns,
and all of this was entirely unnecessary.
As a long-time user, I experienced what happened to the basic list as highly disruptive, and
I won't be told by anyone who never used the basic list in Unicode 
for their own work and who has no stake in the stability of the basic list that this was all
perfectly normal and acceptable.

<h2>Atomic glyphs of overlays</h2>

In the case of, for example, U+130C1 (forearm and leg overlayed; see above), 
one will find inscriptions where the forearm is depicted as being over the leg, 
inscriptions where the leg is depicted as being over the forearm, as well 
as inscriptions where the "z-order" is indeterminate, that is, where 
the curves of the forearm and leg simply cross one another.
This is a palaeographic matter, and therefore Unicode appropriately does not attempt to
encode these different graphical realizations, 
and the single code point U+130C1 is meant to cover all three cases.
<p>
Nonetheless, there are advantages to keeping the z-order of glyphs 
in the code charts of such composite
glyphs unchanged (in the absence of any compelling reasons for any change). 
This has to do with hieroglyphic processing
tools that are built on top of the Unicode sign list 
and that provide support for a more faithful specification of the positioning of
signs in a hieroglyphic inscription than would ever be possible in Unicode. I'm in particular referring to
<a href="https://mjn.host.cs.st-andrews.ac.uk/egyptian/res/">RES</a>, which was
mentioned before. It allows scaling of signs, fine-tuning of distances between signs, 
arbitrary degrees of rotation, and so on.
RES also allows specification of z-order of overlays in the case of compositional encoding. 
The reason RES can do this is because its three implementations are in general-purpose
programming languages (C, Java, JavaScript), in which one can implement graphical algorithms
that are beyond the power of typical Unicode technology such as OpenType.
For example, the Unicode 5.2 glyph of U+130C1 can be expressed in RES as:
<blockquote>
stack[on](D36,D58)
</blockquote>
whereas its Unicode 16 glyph corresponds to:
<blockquote>
stack[under](D36,D58)
</blockquote>
The indeterminate form, where conceptually the two signs have the same z-order, 
is expressed by:
<blockquote>
stack(D36,D58)
</blockquote>
Because U+130C1 has an individual name D59 for the atomically encoded overlay, derived from the
UniKemet names of the Unicode 5.2 set, the first of these could alternatively be encoded as:
<blockquote>
D59
</blockquote>
Regrettably, with a font created according to the Unicode 16 code charts,
D59 has now become equivalent to stack[under](D36,D58) instead.
It is very likely that some RES encodings created in the past will thereby have become 
inconsistent with the intentions of the encoders.
<p>
I don't see any downsides to reverting to the Unicode 5.2 forms for composite signs, 
all the more as the Database offers no evidence in favour of any of
the variant graphical realizations with different z-orders, implying that 
there was never any compelling reason to depart from the Unicode 5.2 forms to begin with.
Moreover, composite signs such as U+130C1 were declared "Legacy" in favour of compositional
encoding using the OVERLAY control, and therefore the glyphs of such signs will not affect
future users who intend to avoid legacy signs altogether.
<p>
Incidentally, the motivation for designing RES was to address the problem that
the longevity of hieroglyphic encodings is very limited with
traditional means, and changing to a different font, 
changing to a different tool, or even changing to a different version of the same tool
often meant that an existing encoding lost its validity.
This has arguably plagued the field for decades. For background see
<a href="https://mjn.host.cs.st-andrews.ac.uk/publications/2013b.pdf">The Manuel de Codage encoding of hieroglyphs impedes development of corpora</a> and
<a href="https://mjn.host.cs.st-andrews.ac.uk/publications/2021a.pdf">Formatting of Ancient Egyptian hieroglyphic text</a>.
<p>
The reason why RES was designed on top of Unicode was because at the time
it was anticipated that Unicode would provide better stability than the various versions
of the Manuel de Codage signs lists and the Hieroglyphica and their implementations
in various tools. The trust that I placed in Unicode at the time
was that it would not fundamentally change characters without compelling reasons at the very least,
in the light of Unicode's stability policies.
